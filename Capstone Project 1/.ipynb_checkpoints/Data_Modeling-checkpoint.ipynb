{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Loan Data \n",
    "\n",
    "### Summary\n",
    "**Big Picture Summary:** The purpose of this notebook is to develop a machine learning model for the conservative investor in Lending Club. The data used in this notebook was cleaned in the [Data_Wrangling.ipynb](https://github.com/paulb17/Springboard/blob/master/Capstone%20Project%201/Data_Wrangling%20.ipynb) notebook. Following this the data was explored in the notebook [Data_Exploration.ipynb](https://github.com/paulb17/Springboard/blob/master/Capstone%20Project%201/Data_Exploration.ipynb). \n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# importing useful functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict, KFold, train_test_split \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "\n",
    "# creating plots using seaborn setting \n",
    "sns.set()\n",
    "\n",
    "# using jupyter magic to display plots in line\n",
    "%matplotlib inline\n",
    "\n",
    "# importing the dataset\n",
    "loan_data = pd.read_csv('Wrangled_Loan_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "Prior to creating machine learning models, it is important to determine a measure of accuracy that will be used to compaere it with other models. Below we outline the metric chosen for comparing the data.\n",
    "\n",
    "### Method of Comparison\n",
    "\n",
    "The sensitivity and specificity will be used as metrics to determine how worthwhile the final model is to the conservative investor. For this problem, sensitivy is interpreted as the number of loans that the model correctly predicts will be fully paid as a percentage of the total number of loans that is fully paid. Addtionally, specificity is the number of loans that the model incorrectly predicts is fully paid as a percentage of the total number of loans that are charged off.\n",
    "\n",
    "Since this problem is viewed from the standpoint of a conservative investor, false positives should be treated differently than false negatives. Conservative investors would want to minimize risk, and avoid false positives as much as possible: they would not mind missing out on opportunities (false negatives) as much as they would mind funding a risky loan (false positives). Consequently, the specificity metric will be more important than the sensitivity metric.\n",
    "\n",
    "In order to identify the best model to use for the conservative investor each machine learning algorithm will be compared using the AUROC (Area Under the Receiver Operating Characteristics)\n",
    "\n",
    "### Creating the baseline model\n",
    "A logistic regression model using the default sklearn parameters will serve as a baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training set the ROCAUC is:0.500978236157\n",
      "\n",
      "For the test set the ROCAUC is:0.500299868073\n"
     ]
    }
   ],
   "source": [
    "# creating list of features\n",
    "loan_features = loan_data.drop(['loan_status'], axis =1)\n",
    "features = list(loan_features.columns)\n",
    "\n",
    "# train test set split\n",
    "X_train, X_test, y_train, y_test = train_test_split(loan_data[features], loan_data[\"loan_status\"], \n",
    "                                                    train_size = 0.75, test_size = 0.25, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# instantiating baseline logistic regression model\n",
    "base_lr_model = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "# setting the number of folds\n",
    "kf = KFold(10, random_state = 1)\n",
    "    \n",
    "# fitting the model and computing predictions\n",
    "base_lr_model.fit(X_train, y_train)\n",
    "base_prediction_train = cross_val_predict(base_lr_model, X_train, y_train, cv=kf)\n",
    "\n",
    "# calcluating the AUROC for the training set\n",
    "base_train_roc = roc_auc_score(y_train, base_prediction_train)\n",
    "print('For the training set the ROCAUC is:' + str(base_train_roc)+ '\\n' )\n",
    "\n",
    "# predicting results on test set\n",
    "base_prediction_test = base_lr_model.predict(X_test)\n",
    "\n",
    "# calculating the AUROC for the test set\n",
    "base_test_roc = roc_auc_score(y_test, base_prediction_test)\n",
    "print('For the test set the ROCAUC is:' + str(base_test_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROCAUC score of the baseline model indicates that it is unable to distinguish charged off and fully paid loans. Below the confusion matrix of the training and test set predictions are shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training set the confusion matrix is:\n",
      "[[   10  4249]\n",
      " [   10 25533]]\n",
      "\n",
      "\n",
      "For the test set the confusion matrix is:\n",
      "[[   1 1394]\n",
      " [   1 8538]]\n"
     ]
    }
   ],
   "source": [
    "# creating confusion matrix for the training set \n",
    "cm = confusion_matrix(y_train, base_prediction_train)\n",
    "print('For the training set the confusion matrix is:' + '\\n' + str(cm))\n",
    "       \n",
    "# create confusion matrix for the test set\n",
    "cm= confusion_matrix(y_test, base_prediction_test)\n",
    "print('\\n'*2 + 'For the test set' + ' the confusion matrix is:' + '\\n'+ str(cm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most of the borrowers in the baseline model are predicted to fully pay off their loans. This is as a result of the class imbalance in the dataset. There are 6 times more borrowers with loans that were paid off on time (1), than there are with loans that were charged off (0). To account for this, a grid search will be carried out using different weights. \n",
    "\n",
    "Below, we start by creating a function to implement this gridseearch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a grid search function to fit and test models\n",
    "The function created below takes in a classification model and tunes hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(model, param_grid, score = 'roc_auc'):\n",
    "    \n",
    "    # setting the number of folds\n",
    "    kf = KFold(10, random_state=1)\n",
    "    \n",
    "    # Instantiate the GridSearchCV object: cv\n",
    "    model_cv = GridSearchCV(model, param_grid, cv = kf, scoring = score, \n",
    "                            return_train_score = True)\n",
    "    \n",
    "    with parallel_backend('threading'):\n",
    "        # Fitting the training set \n",
    "        model_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict the labels of the test set: y_pred        \n",
    "        y_pred = model_cv.predict(X_test)\n",
    "\n",
    "\n",
    "    # Compute and print metrics\n",
    "    print(\"Tuned Model Parameters: {}\".format(model_cv.best_params_))\n",
    "    \n",
    "    return pd.DataFrame(model_cv.cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with varying class weights and regularization parameter\n",
    "Below we tune the class weights to account for the class imbalance in the dataset. Furthermore, we also attempt to tune the regularization parameter to get a better understanding of its effect on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Parameters: {'C': 100.0, 'class_weight': {0: 7, 1: 1}}\n",
      "CPU times: user 12min 50s, sys: 24.2 s, total: 13min 14s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "# c _space\n",
    "c_space = np.logspace(-3, 3, 7)\n",
    "\n",
    "# parameters for grid search \n",
    "param_grid = {'C': c_space, 'class_weight':[{0:5, 1:1}, {0:6, 1:1}, 'balanced', {0:7,1:1}, {0:8,1:1}]}\n",
    "\n",
    "# calculating results\n",
    "%time results = classification_model(base_lr_model, param_grid)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 100.0, 'class_weight': {0: 7, 1: 1}}</td>\n",
       "      <td>0.702363</td>\n",
       "      <td>0.708108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>{'C': 1.0, 'class_weight': {0: 7, 1: 1}}</td>\n",
       "      <td>0.702309</td>\n",
       "      <td>0.708546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>{'C': 0.01, 'class_weight': {0: 7, 1: 1}}</td>\n",
       "      <td>0.702177</td>\n",
       "      <td>0.707658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>{'C': 1000.0, 'class_weight': {0: 7, 1: 1}}</td>\n",
       "      <td>0.701619</td>\n",
       "      <td>0.708253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 7, 1: 1}}</td>\n",
       "      <td>0.701601</td>\n",
       "      <td>0.708452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>{'C': 10.0, 'class_weight': {0: 7, 1: 1}}</td>\n",
       "      <td>0.701329</td>\n",
       "      <td>0.707336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>{'C': 10.0, 'class_weight': 'balanced'}</td>\n",
       "      <td>0.701035</td>\n",
       "      <td>0.705778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>{'C': 0.1, 'class_weight': {0: 6, 1: 1}}</td>\n",
       "      <td>0.700066</td>\n",
       "      <td>0.705039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9</td>\n",
       "      <td>{'C': 1000.0, 'class_weight': 'balanced'}</td>\n",
       "      <td>0.700043</td>\n",
       "      <td>0.705415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 0.01, 'class_weight': {0: 6, 1: 1}}</td>\n",
       "      <td>0.700005</td>\n",
       "      <td>0.704540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score                                       params  \\\n",
       "28                1   {'C': 100.0, 'class_weight': {0: 7, 1: 1}}   \n",
       "18                2     {'C': 1.0, 'class_weight': {0: 7, 1: 1}}   \n",
       "8                 3    {'C': 0.01, 'class_weight': {0: 7, 1: 1}}   \n",
       "33                4  {'C': 1000.0, 'class_weight': {0: 7, 1: 1}}   \n",
       "13                5     {'C': 0.1, 'class_weight': {0: 7, 1: 1}}   \n",
       "23                6    {'C': 10.0, 'class_weight': {0: 7, 1: 1}}   \n",
       "22                7      {'C': 10.0, 'class_weight': 'balanced'}   \n",
       "11                8     {'C': 0.1, 'class_weight': {0: 6, 1: 1}}   \n",
       "32                9    {'C': 1000.0, 'class_weight': 'balanced'}   \n",
       "6                10    {'C': 0.01, 'class_weight': {0: 6, 1: 1}}   \n",
       "\n",
       "    mean_test_score  mean_train_score  \n",
       "28         0.702363          0.708108  \n",
       "18         0.702309          0.708546  \n",
       "8          0.702177          0.707658  \n",
       "33         0.701619          0.708253  \n",
       "13         0.701601          0.708452  \n",
       "23         0.701329          0.707336  \n",
       "22         0.701035          0.705778  \n",
       "11         0.700066          0.705039  \n",
       "32         0.700043          0.705415  \n",
       "6          0.700005          0.704540  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns of importance\n",
    "result_columns = ['rank_test_score', 'params', 'mean_test_score', 'mean_train_score']\n",
    "results[result_columns].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that having the penalty for misclassifying charged off loans to be 7 times the penalty for misclassifying fully paid loans appears to have the best AUROC values on average. In addition, we note that while varying the regularization parameter \"C\" has an affect on ROCAUC values, the differences it makes in AUROC scores is small. \n",
    "\n",
    "The overall best AUROC value on the test set belongs to the model with {'C': 1.0, 'class_weight': {0: 7, 1: 1}}. The value is 0.702. Let's look into whether another model could further improve the AUROC of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification\n",
    "There are a lot of parameters that may be worth tuning. To investigate this, we start by creating a function to randomly search through a selected grid of random forest parameters. Following this, the parameters that appear to be the most important will then be tuned using grid search.   \n",
    "\n",
    "**Creating function for randomized searching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model, random_grid, score = 'roc_auc', iterations = 30):\n",
    "    \n",
    "    # setting the number of folds\n",
    "    kf = KFold(10, random_state=1)\n",
    "    \n",
    "    # Instantiate the GridSearchCV object: cv\n",
    "    model_cv = RandomizedSearchCV(model, random_grid, n_iter = iterations, cv = kf, \n",
    "                                  scoring = score, return_train_score = True, n_jobs=-3)\n",
    "    \n",
    "\n",
    "    # Fitting the training set \n",
    "    model_cv.fit(X_train, y_train)\n",
    "        \n",
    "    # Predict the labels of the test set: y_pred        \n",
    "    y_pred = model_cv.predict(X_test)\n",
    "\n",
    "    # Compute and print metrics\n",
    "    print(\"Tuned Model Parameters: {}\".format(model_cv.best_params_))\n",
    "    \n",
    "    return pd.DataFrame(model_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'auto', 'max_depth': 200, 'class_weight': 'balanced', 'bootstrap': True}\n",
      "CPU times: user 12.2 s, sys: 736 ms, total: 12.9 s\n",
      "Wall time: 28min 25s\n"
     ]
    }
   ],
   "source": [
    "# creating model object and evaluating result \n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# parameters for grid search \n",
    "random_grid = {'class_weight':[{0:6, 1:1}, 'balanced', {0:7,1:1}],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [50, 100, 200, None],\n",
    "               'min_samples_split': [2, 5, 8],\n",
    "               'min_samples_leaf': [1, 2, 5],\n",
    "               'n_estimators': [200, 250],\n",
    "               'bootstrap' : [True, False]}\n",
    "\n",
    "# calculating results\n",
    "%time random_rf_results = random_search(rf_model, random_grid, iterations = 20)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'auto', 'max_depth': 200, 'class_weight': 'balanced', 'bootstrap': True}</td>\n",
       "      <td>0.700272</td>\n",
       "      <td>0.996171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 6, 1: 1}, 'bootstrap': True}</td>\n",
       "      <td>0.700234</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}</td>\n",
       "      <td>0.699625</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}</td>\n",
       "      <td>0.699134</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': 'balanced', 'bootstrap': True}</td>\n",
       "      <td>0.698953</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 200, 'class_weight': {0: 7, 1: 1}, 'bootstrap': True}</td>\n",
       "      <td>0.697327</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 200, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}</td>\n",
       "      <td>0.697214</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 6, 1: 1}, 'bootstrap': True}</td>\n",
       "      <td>0.697197</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 7, 1: 1}, 'bootstrap': True}</td>\n",
       "      <td>0.696939</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': {0: 7, 1: 1}, 'bootstrap': True}</td>\n",
       "      <td>0.696299</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 50, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}</td>\n",
       "      <td>0.696166</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 200, 'class_weight': 'balanced', 'bootstrap': False}</td>\n",
       "      <td>0.695938</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': {0: 7, 1: 1}, 'bootstrap': False}</td>\n",
       "      <td>0.695720</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 50, 'class_weight': 'balanced', 'bootstrap': False}</td>\n",
       "      <td>0.695382</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>{'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': 'balanced', 'bootstrap': False}</td>\n",
       "      <td>0.695280</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None, 'class_weight': {0: 6, 1: 1}, 'bootstrap': True}</td>\n",
       "      <td>0.694853</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 7, 1: 1}, 'bootstrap': True}</td>\n",
       "      <td>0.693856</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}</td>\n",
       "      <td>0.692080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 50, 'class_weight': 'balanced', 'bootstrap': True}</td>\n",
       "      <td>0.691214</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 6, 1: 1}, 'bootstrap': True}</td>\n",
       "      <td>0.691183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  \\\n",
       "19                1   \n",
       "4                 2   \n",
       "6                 3   \n",
       "7                 4   \n",
       "9                 5   \n",
       "10                6   \n",
       "0                 7   \n",
       "18                8   \n",
       "5                 9   \n",
       "14               10   \n",
       "1                11   \n",
       "3                12   \n",
       "16               13   \n",
       "17               14   \n",
       "15               15   \n",
       "12               16   \n",
       "8                17   \n",
       "13               18   \n",
       "11               19   \n",
       "2                20   \n",
       "\n",
       "                                                                                                                                                               params  \\\n",
       "19      {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'auto', 'max_depth': 200, 'class_weight': 'balanced', 'bootstrap': True}   \n",
       "4     {'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 6, 1: 1}, 'bootstrap': True}   \n",
       "6   {'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}   \n",
       "7    {'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}   \n",
       "9      {'n_estimators': 250, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': 'balanced', 'bootstrap': True}   \n",
       "10    {'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 200, 'class_weight': {0: 7, 1: 1}, 'bootstrap': True}   \n",
       "0    {'n_estimators': 200, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 200, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}   \n",
       "18    {'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 6, 1: 1}, 'bootstrap': True}   \n",
       "5     {'n_estimators': 250, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 7, 1: 1}, 'bootstrap': True}   \n",
       "14   {'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': {0: 7, 1: 1}, 'bootstrap': True}   \n",
       "1     {'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 50, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}   \n",
       "3      {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 200, 'class_weight': 'balanced', 'bootstrap': False}   \n",
       "16  {'n_estimators': 200, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': {0: 7, 1: 1}, 'bootstrap': False}   \n",
       "17      {'n_estimators': 250, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 50, 'class_weight': 'balanced', 'bootstrap': False}   \n",
       "15    {'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': 'balanced', 'bootstrap': False}   \n",
       "12   {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None, 'class_weight': {0: 6, 1: 1}, 'bootstrap': True}   \n",
       "8     {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 7, 1: 1}, 'bootstrap': True}   \n",
       "13  {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': {0: 6, 1: 1}, 'bootstrap': False}   \n",
       "11       {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 50, 'class_weight': 'balanced', 'bootstrap': True}   \n",
       "2     {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'class_weight': {0: 6, 1: 1}, 'bootstrap': True}   \n",
       "\n",
       "    mean_test_score  mean_train_score  \n",
       "19         0.700272          0.996171  \n",
       "4          0.700234          0.999999  \n",
       "6          0.699625          0.999998  \n",
       "7          0.699134          0.999999  \n",
       "9          0.698953          1.000000  \n",
       "10         0.697327          1.000000  \n",
       "0          0.697214          1.000000  \n",
       "18         0.697197          1.000000  \n",
       "5          0.696939          1.000000  \n",
       "14         0.696299          1.000000  \n",
       "1          0.696166          1.000000  \n",
       "3          0.695938          1.000000  \n",
       "16         0.695720          1.000000  \n",
       "17         0.695382          1.000000  \n",
       "15         0.695280          1.000000  \n",
       "12         0.694853          1.000000  \n",
       "8          0.693856          1.000000  \n",
       "13         0.692080          1.000000  \n",
       "11         0.691214          1.000000  \n",
       "2          0.691183          1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "random_rf_results[result_columns].sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the training set the ROCAUC is:0.65035295325\n",
      "\n",
      "For the test set the ROCAUC is:0.5\n"
     ]
    }
   ],
   "source": [
    "# creating list of features\n",
    "loan_features = loan_data.drop(['loan_status'], axis =1)\n",
    "features = list(loan_features.columns)\n",
    "log_features = loan_features.copy()\n",
    "log_features['annual_inc'] = np.log(log_features['annual_inc'])\n",
    "\n",
    "# train test set split on log features\n",
    "logX_train, logX_test, logy_train, logy_test = train_test_split(log_features, loan_data[\"loan_status\"], \n",
    "                                                    train_size = 0.75, test_size = 0.25, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# instantiating baseline logistic regression model\n",
    "log_lr_model = LogisticRegression(class_weight= 'balanced')\n",
    "\n",
    "# setting the number of folds\n",
    "kf = KFold(10, random_state = 1)\n",
    "    \n",
    "# fitting the model and computing predictions\n",
    "log_lr_model.fit(logX_train, logy_train)\n",
    "log_prediction_train = cross_val_predict(log_lr_model, logX_train, logy_train, cv=kf)\n",
    "\n",
    "# calcluating the AUROC for the training set\n",
    "log_train_roc = roc_auc_score(logy_train, log_prediction_train)\n",
    "print('For the training set the ROCAUC is:' + str(log_train_roc)+ '\\n' )\n",
    "\n",
    "# predicting results on test set\n",
    "log_prediction_test = log_lr_model.predict(X_test)\n",
    "\n",
    "# calculating the AUROC for the test set\n",
    "log_test_roc = roc_auc_score(logy_test, log_prediction_test)\n",
    "print('For the test set the ROCAUC is:' + str(log_test_roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix for the training set \n",
    "cm = confusion_matrix(y_train, predictions_train)\n",
    "print('For the training set the confusion matrix is:' + '\\n' + str(cm))\n",
    "\n",
    "# calculating sensitivity for the training set\n",
    "sensitivity_train = cm[1,1]/(cm[1,1]+cm[1,0])\n",
    "print('The sensitivity of the training set is: ' + str(sensitivity_train))\n",
    "\n",
    "# calculating specificity for the training set\n",
    "specificity_train = cm[0,1]/(cm[0,1]+cm[0,0])\n",
    "print('The specificity of the training set is: ' + str(specificity_train))\n",
    "\n",
    "\n",
    "    \n",
    "# predicting results on test set\n",
    "prediction_test = base_lr_model.predict(X_test)\n",
    "       \n",
    "# create confusion matrix for the test set\n",
    "cm= confusion_matrix(y_test, prediction_test)\n",
    "print('\\n'*2 + 'For the test set' + ' the confusion matrix is:' + '\\n'+ str(cm))\n",
    "\n",
    "# calculating sensitivity of the test set\n",
    "sensitivity_test = cm[1,1]/(cm[1,1]+cm[1,0])\n",
    "print('The sensitivity of the test set is: ' + str(sensitivity_test))\n",
    "\n",
    "# calculating specificity of the test set\n",
    "specificity_test = cm[0,1]/(cm[0,1]+cm[0,0])\n",
    "print('The specificity of the test set is: ' + str(specificity_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
